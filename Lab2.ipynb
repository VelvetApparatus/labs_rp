{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba0dcd2-cc93-44a1-bfa6-05cd9ff1f65b",
   "metadata": {},
   "source": [
    "# Лабаораторная работа №2: Классификация аудио"
   ]
  },
  {
   "cell_type": "code",
   "id": "421ba5b3-d1be-42c6-b9c4-75b4a4fc3dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:08:58.185033Z",
     "start_time": "2025-11-30T21:08:51.989951Z"
    }
   },
   "source": [
    "!pip -q install datasets torchaudio torch torchvision scikit-learn transformers accelerate --upgrade\n",
    "\n",
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio.transforms as T\n",
    "from datasets import load_dataset, Audio\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "SR = 16000\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/velimirhlebnikov/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6c914fbb-38a0-4362-8553-d7d36b936c14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:09:03.079579Z",
     "start_time": "2025-11-30T21:08:58.210212Z"
    }
   },
   "source": [
    "ds = load_dataset(\"danavery/urbansound8K\")\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=SR))\n",
    "\n",
    "def by_fold(d, folds): return d.filter(lambda ex: ex[\"fold\"] in folds)\n",
    "train_raw = by_fold(ds[\"train\"], list(range(1,9)))   # 1..8\n",
    "valid_raw = by_fold(ds[\"train\"], [9])                # 9\n",
    "test_raw  = by_fold(ds[\"train\"], [10])               # 10\n",
    "\n",
    "labels = sorted(list(set(train_raw[\"class\"])))\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "label_col = \"class\"\n",
    "\n",
    "len(train_raw), len(valid_raw), len(test_raw), len(labels)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7079, 816, 837, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ce166c4b-c98e-417c-b2e0-8e354c66db4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:09:04.663792Z",
     "start_time": "2025-11-30T21:09:03.160752Z"
    }
   },
   "source": [
    "N_MELS, N_FFT, HOP = 64, 1024, 256\n",
    "FMIN, FMAX = 20.0, SR/2\n",
    "mel = T.MelSpectrogram(sample_rate=SR, n_fft=N_FFT, hop_length=HOP,\n",
    "                       n_mels=N_MELS, f_min=FMIN, f_max=FMAX, power=2.0)\n",
    "to_db = T.AmplitudeToDB(top_db=80)\n",
    "\n",
    "def logmel_stats(ex):\n",
    "    wav = torch.tensor(ex[\"audio\"][\"array\"], dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        S = to_db(mel(wav)).squeeze(0).numpy().astype(\"float32\")  # (M,T)\n",
    "    m, s = S.mean(), S.std() + 1e-6\n",
    "    S = (S - m) / s\n",
    "    feat = np.concatenate([S.mean(1), S.std(1)]).astype(\"float32\")\n",
    "    return {\"feat\": feat, \"y\": label2id[ex[label_col]]}\n",
    "\n",
    "def to_xy(split):\n",
    "    a = split.map(logmel_stats, remove_columns=split.column_names)\n",
    "    return np.stack(a[\"feat\"]), np.array(a[\"y\"])\n",
    "\n",
    "X_tr, y_tr = to_xy(train_raw)\n",
    "X_va, y_va = to_xy(valid_raw)\n",
    "X_te, y_te = to_xy(test_raw)\n",
    "\n",
    "clf = Pipeline([(\"scaler\", StandardScaler()), (\"svm\", LinearSVC())])\n",
    "clf.fit(X_tr, y_tr)\n",
    "print(\"LinearSVC  VAL acc:\", accuracy_score(y_va, clf.predict(X_va)))\n",
    "print(\"LinearSVC  TEST acc:\", accuracy_score(y_te, clf.predict(X_te)))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC  VAL acc: 0.5625\n",
      "LinearSVC  TEST acc: 0.5197132616487455\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "50a98a92-94fc-4d9d-a797-8727f44d45e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:09:52.505529Z",
     "start_time": "2025-11-30T21:09:04.671587Z"
    }
   },
   "source": [
    "class RawWaveDS(Dataset):\n",
    "    def __init__(self, split): self.s = split\n",
    "    def __len__(self): return len(self.s)\n",
    "    def __getitem__(self, i):\n",
    "        ex = self.s[i]\n",
    "        return torch.tensor(ex[\"audio\"][\"array\"], dtype=torch.float32), label2id[ex[label_col]]\n",
    "\n",
    "def pad_collate(batch):\n",
    "    waves, ys = zip(*batch)\n",
    "    L = max(w.shape[0] for w in waves)\n",
    "    X = torch.zeros(len(waves), L)\n",
    "    for i,w in enumerate(waves): X[i,:w.shape[0]] = w\n",
    "    return X, torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(RawWaveDS(train_raw), batch_size=32, shuffle=True,  num_workers=0, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader(RawWaveDS(valid_raw), batch_size=32, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(RawWaveDS(test_raw),  batch_size=32, shuffle=False, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "mel_gpu = T.MelSpectrogram(sample_rate=SR, n_fft=N_FFT, hop_length=HOP,\n",
    "                           n_mels=N_MELS, f_min=FMIN, f_max=FMAX, power=2.0).to(device)\n",
    "to_db_gpu = T.AmplitudeToDB(top_db=80).to(device)\n",
    "FIX_T = 128\n",
    "def to_logmel_batch(xb_1d):\n",
    "    S = to_db_gpu(mel_gpu(xb_1d))          # (B,M,Tm)\n",
    "    m = S.mean(dim=(1,2), keepdim=True); s = S.std(dim=(1,2), keepdim=True).clamp_min(1e-6)\n",
    "    S = (S - m)/s\n",
    "    Tm = S.size(-1)\n",
    "    if Tm < FIX_T: S = F.pad(S, (0, FIX_T-Tm))\n",
    "    else: S = S[:, :, :FIX_T]\n",
    "    return S.unsqueeze(1)                  # (B,1,M,FIX_T)\n",
    "\n",
    "class VGGishAudio(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.h(self.f(x))\n",
    "\n",
    "\n",
    "model = VGGishAudio(len(label2id)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_epoch(loader, train=True, tag=\"\"):\n",
    "    model.train() if train else model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for xb_wave, yb in tqdm(loader, desc=tag, leave=False):\n",
    "        xb_wave, yb = xb_wave.to(device), yb.to(device)\n",
    "        xb = to_logmel_batch(xb_wave)\n",
    "        if train: opt.zero_grad(set_to_none=True)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(xb); loss = crit(logits, yb)\n",
    "            if train: loss.backward(); opt.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "        pred = logits.argmax(1); correct += (pred==yb).sum().item(); total += xb.size(0)\n",
    "    return loss_sum/total, correct/total\n",
    "epochs = 2\n",
    "for ep in range(1, epochs):\n",
    "    tr_l, tr_a = run_epoch(train_loader, True,  f\"train {ep}/{epochs}\")\n",
    "    va_l, va_a = run_epoch(val_loader,   False, f\"valid {ep}/{epochs}\")\n",
    "    print(f\"Ep {ep:02d}: train {tr_l:.4f}/{tr_a:.3f} | valid {va_l:.4f}/{va_a:.3f}\")\n",
    "\n",
    "# Тест\n",
    "model.eval(); y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb_wave, yb in tqdm(test_loader, desc=\"test\", leave=False):\n",
    "        logits = model(to_logmel_batch(xb_wave.to(device)))\n",
    "        y_true += yb.numpy().tolist()\n",
    "        y_pred += logits.argmax(1).cpu().numpy().tolist()\n",
    "print(\"CNN TEST acc:\", accuracy_score(y_true, y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 01: train 1.3945/0.506 | valid 1.2969/0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN TEST acc: 0.45280764635603343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a13d71f3-0b08-4a6f-9f94-86d55a83d5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T21:19:29.319182Z",
     "start_time": "2025-11-30T21:11:27.715426Z"
    }
   },
   "source": [
    "MODEL_ID = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "ast = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)\n",
    "\n",
    "def map_raw(ex):\n",
    "    return {\n",
    "        \"wave\": np.asarray(ex[\"audio\"][\"array\"], dtype=\"float32\"),\n",
    "        \"labels\": label2id[ex[label_col]],\n",
    "    }\n",
    "\n",
    "tr_ast = train_raw.map(map_raw, remove_columns=train_raw.column_names, load_from_cache_file=False)\n",
    "va_ast = valid_raw.map(map_raw, remove_columns=valid_raw.column_names, load_from_cache_file=False)\n",
    "te_ast = test_raw .map(map_raw, remove_columns=test_raw .column_names, load_from_cache_file=False)\n",
    "\n",
    "\n",
    "\n",
    "def collate_proc(batch):\n",
    "    def extract_wave(b):\n",
    "        if \"wave\" in b:\n",
    "            return b[\"wave\"]\n",
    "        if \"audio\" in b and isinstance(b[\"audio\"], dict) and \"array\" in b[\"audio\"]:\n",
    "            return b[\"audio\"][\"array\"]\n",
    "        if \"input_values\" in b: \n",
    "            return b[\"input_values\"]\n",
    "        raise KeyError(f\"Expected one of keys ['wave','audio','input_values'], got {list(b.keys())}\")\n",
    "\n",
    "    waves  = [extract_wave(b) for b in batch]\n",
    "    labels = [b[\"labels\"] if \"labels\" in b else b[\"label\"] for b in batch]\n",
    "\n",
    "    inputs = processor(waves, sampling_rate=SR, return_tensors=\"pt\",\n",
    "                       padding=True, truncation=True)\n",
    "    inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, -1)\n",
    "    return {\"accuracy\": float(accuracy_score(labels, preds)),\n",
    "            \"f1_macro\": float(f1_score(labels, preds, average=\"macro\"))}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ast_us8k_ft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_steps=500,\n",
    "    logging_steps=200,\n",
    "    fp16=(device.type == \"cuda\"),\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ast,\n",
    "    args=args,\n",
    "    train_dataset=tr_ast,\n",
    "    eval_dataset=va_ast,\n",
    "    data_collator=collate_proc,    \n",
    "    processing_class=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"AST VALID:\", trainer.evaluate(va_ast))\n",
    "print(\"AST TEST :\", trainer.evaluate(te_ast))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 7079/7079 [00:30<00:00, 232.93 examples/s]\n",
      "Map: 100%|██████████| 816/816 [00:03<00:00, 217.73 examples/s]\n",
      "Map: 100%|██████████| 837/837 [00:03<00:00, 215.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='592' max='35400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  592/35400 07:17 < 7:09:54, 1.35 it/s, Epoch 0.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ae0b403f31a827b78425e8722450c38b"
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 77\u001B[39m\n\u001B[32m     49\u001B[39m args = TrainingArguments(\n\u001B[32m     50\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33mast_us8k_ft\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     51\u001B[39m     per_device_train_batch_size=\u001B[32m2\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     63\u001B[39m     dataloader_pin_memory=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     64\u001B[39m )\n\u001B[32m     67\u001B[39m trainer = Trainer(\n\u001B[32m     68\u001B[39m     model=ast,\n\u001B[32m     69\u001B[39m     args=args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     74\u001B[39m     compute_metrics=compute_metrics,\n\u001B[32m     75\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     78\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mAST VALID:\u001B[39m\u001B[33m\"\u001B[39m, trainer.evaluate(va_ast))\n\u001B[32m     79\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mAST TEST :\u001B[39m\u001B[33m\"\u001B[39m, trainer.evaluate(te_ast))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/transformers/trainer.py:2674\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2667\u001B[39m context = (\n\u001B[32m   2668\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2669\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2670\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2671\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2672\u001B[39m )\n\u001B[32m   2673\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2674\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2677\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2678\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2679\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2680\u001B[39m ):\n\u001B[32m   2681\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2682\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/transformers/trainer.py:4071\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   4068\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   4069\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4071\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4073\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:2740\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2738\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2739\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2740\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/МТУСИ/project_prcatice/speech-intencity/audio-events-detection/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "38be72f1-4cc9-4f44-a0e7-64d0513c2d30",
   "metadata": {},
   "source": [
    "## Задание 1. Таблица сравнения и графики обучения (CNN vs AST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7e710-fca0-4c3a-96da-139f2c44f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(1.1): Таблица сравнения (VAL/TEST) для CNN и AST\n",
    "# Подставьте ваши значения метрик. Если их нет — посчитайте/извлеките выше.\n",
    "# Требуемые поля: cnn_val_acc, cnn_val_f1, cnn_test_acc, cnn_test_f1\n",
    "#                 ast_val_acc, ast_val_f1, ast_test_acc, ast_test_f1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Пример: замените None на ваши переменные/числа ---\n",
    "cnn_val_acc  = None  # TODO\n",
    "cnn_val_f1   = None  # TODO\n",
    "cnn_test_acc = None  # TODO\n",
    "cnn_test_f1  = None  # TODO\n",
    "\n",
    "ast_val_acc  = None  # TODO\n",
    "ast_val_f1   = None  # TODO\n",
    "ast_test_acc = None  # TODO\n",
    "ast_test_f1  = None  # TODO\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    [\"CNN (VGG)\", cnn_val_acc, cnn_val_f1, cnn_test_acc, cnn_test_f1],\n",
    "    [\"AST FT\",    ast_val_acc, ast_val_f1, ast_test_acc, ast_test_f1],\n",
    "], columns=[\"Model\", \"VAL acc\", \"VAL f1_macro\", \"TEST acc\", \"TEST f1_macro\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb85842-d1d3-4356-8d56-d675d50da56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(1.2): Графики обучения CNN (loss/acc по эпохам)\n",
    "#  Логгируйте значения из вашего цикла обучения.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Пример построения (раскомментируйте и подставьте):\n",
    "# plt.figure(); plt.plot(history[\"tr_loss\"]); plt.plot(history[\"va_loss\"]); \n",
    "# plt.title(\"CNN — Loss\"); plt.legend([\"train\",\"valid\"]); plt.xlabel(\"epoch\"); plt.show()\n",
    "# plt.figure(); plt.plot(history[\"tr_acc\"]); plt.plot(history[\"va_acc\"]);\n",
    "# plt.title(\"CNN — Accuracy\"); plt.legend([\"train\",\"valid\"]); plt.xlabel(\"epoch\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f29c69-aecf-44f8-a163-1ebf04147840",
   "metadata": {},
   "source": [
    "## Задание 2. Эксперимент (выберите один вариант)\n",
    "\n",
    "**Вариант A:** добавить SpecAugment в обучение CNN.  \n",
    "**Вариант B:** изменить параметры мел-спектрограмм (например, `N_MELS`, `HOP`, `N_FFT`) и переобучить CNN.\n",
    "\n",
    "Оформите гипотезу → что меняете → метрики ДО/ПОСЛЕ → краткий вывод.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4f974-2f48-4249-9e0a-907edfa534f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2.A): SpecAugment для CNN\n",
    "# Вставьте вызов в обучающий цикл CNN только для train-batch (до forward).\n",
    "# Подпишите конфигурацию масок.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def specaugment(x, time_mask=20, freq_mask=8, n_time_masks=1, n_freq_masks=1):\n",
    "    \"\"\"\n",
    "    x: (B, 1, n_mels, T) — мел-спектры\n",
    "    Возвращает аугментированный тензор.\n",
    "    \"\"\"\n",
    "    # TODO: реализовать маскирование по времени и по частоте (n_time_masks / n_freq_masks)\n",
    "    # Подсказка: зануляйте x[:, :, f0:f0+f, :] и x[:, :, :, t0:t0+t]\n",
    "    return x\n",
    "\n",
    "# Пример подключения в цикле:\n",
    "# xb = to_logmel_batch(xb_wave)\n",
    "# if train:\n",
    "#     xb = specaugment(xb, time_mask=..., freq_mask=...)\n",
    "# logits = model(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7697d-587a-4522-858c-ab1379557618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(2.B): Изменение мел-параметров для CNN\n",
    "# Укажите НОВЫЕ значения, пересоздайте мел-преобразования и переобучите CNN.\n",
    "\n",
    "# Пример: (замените на свои)\n",
    "# N_MELS_NEW = 80   # было 64\n",
    "# HOP_NEW    = 160  # было 256\n",
    "# N_FFT_NEW  = 1024 # по необходимости\n",
    "\n",
    "# 1) Пересоздайте MelSpectrogram/AmplitudeToDB с новыми параметрами\n",
    "# 2) Обновите функцию to_logmel_batch (если параметры зашиты)\n",
    "# 3) Переобучите CNN и посчитаете метрики (VAL/TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f97a97-89c7-4385-921d-0a031c78e0ff",
   "metadata": {},
   "source": [
    "### Отчёт по эксперименту\n",
    "**Вариант:** A (SpecAugment) / B (мел-параметры)  \n",
    "**Гипотеза:** …  \n",
    "**Конфигурация:** …  \n",
    "**Результаты (VAL/TEST, acc и macro-F1):** ДО → … | ПОСЛЕ → …  \n",
    "**Вывод (1–3 предложения):** …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc8320-d534-4853-9a7f-299a3a971b02",
   "metadata": {},
   "source": [
    "## Задание 3. Инференс AST на собственных `.wav`\n",
    "Загрузите файл(ы), при необходимости ресемплируйте до 16kHz, сделайте топ-K предсказаний AST и прокомментируйте результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274fb9e-7682-43a9-a3e5-1f3ddf896932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(3): Инференс AST на своём .wav\n",
    "# Требуются: processor, ast (обученная модель), SR, id2label\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "def ast_predict_wav(path, top_k=3):\n",
    "    # 1) загрузка .wav\n",
    "    wav, sr = sf.read(path)\n",
    "    wav = wav.astype(np.float32)\n",
    "\n",
    "    # 2) ресемплинг при несоответствии частоты\n",
    "    #TODO\n",
    "\n",
    "    # 3) препроцессинг\n",
    "    inputs = processor(wav, sampling_rate=SR, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(ast.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 4) инференс\n",
    "    ast.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = ast(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "\n",
    "    # 5) топ-K\n",
    "    vals, idx = torch.topk(probs, k=min(top_k, probs.numel()))\n",
    "    vals, idx = vals.cpu().numpy(), idx.cpu().numpy()\n",
    "    return [(id2label[int(i)], float(v)) for i, v in zip(idx, vals)]\n",
    "\n",
    "# Пример:\n",
    "# ast_predict_wav(\"my_audio.wav\", top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
